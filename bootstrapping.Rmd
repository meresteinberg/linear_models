---
title: "Bootstrapping"
output: github_document
date: "2024-11-18"
---

```{r}
library(tidyverse)
library(modelr)
library(p8105.datasets)


set.seed(1)
```

do some boostrapping!!

make up some data
```{r}
n_samp=250

sim_df_constant=
  tibble(
    x=rnorm(n_samp, 1, 1),
    error= rnorm(n_samp, 0, 1),
    y=2 +3*x + error
  )

sim_df_nonconstant=
  sim_df_constant |> 
  mutate(
    error= error * .75 * x,
     y=2 +3*x + error
  )
```

Lets look at these.
```{r}
sim_df_constant |> 
  ggplot(aes(x=x, y=y))+
  geom_point()+
  stat_smooth(method = "lm")

sim_df_nonconstant |> 
  ggplot(aes(x=x, y=y))+
  geom_point()+
  stat_smooth(method = "lm")
```
when x gets close to zero, residuals get really tight
can see uncertainty put in wrong place based on grey lines

look at regression results
```{r}
sim_df_constant |> 
  lm(y~x, data= _) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)

sim_df_nonconstant |> 
  lm(y~x, data= _) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
```
std error for x and intercept pretty close, when they shouldnt be (we probably know more about intercept)

## Draw a bootstrap sample
```{r}
boot_sample= function(df) {
  
  boot_df=
    sample_frac(df, replace = TRUE) |> 
    arrange(x)
  
  return(boot_df)
}
```

Lets try running this
```{r}
sim_df_nonconstant |> 
  boot_sample() |> 
  ggplot(aes(x=x, y=y))+
  geom_point(alpha=.5)+
  stat_smooth(method="lm")
```
df is arranged by x and we see repeats bc replace=TRUE
some points grey and some darker (repeats)
each bootstrap is giving us a slightly diff regression fit. bootstrap sample should reflect actual variability in data that we have..variability is what we care about

Can we do this as part of an analysis? yes!
```{r}
sim_df_nonconstant |> 
  boot_sample() |> 
   lm(y~x, data= _) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
```


goal is generate 1000 datasets and pull out results

## Bootstrap a LOT

```{r}
boot_straps=
  tibble(
    strap_number=1:1000
  ) |> 
  mutate(
    strap_sample= map(strap_number, \(i) boot_sample(df=sim_df_nonconstant)),
    models=map(strap_sample, \(df) lm(y~x, data=df)),
    results=map(models, broom::tidy)
  )

bootstrap_results=
  boot_straps |> 
  select(strap_number, results) |> 
  unnest(results) |> 
  group_by(term) |> 
  summarize(
    boot_se= sd(estimate)
  ) |> 
  knitr::kable(digits=3)
```
if wanted to look at each one , in console: boot_straps |> pull(strap_sample) |> nth(1)

from bootstrap results...now intercept has lower variability and slope has higher variability compared to nonconstant distribution

if used sim_df_constant, no violation of assumptions, so we would get basically same results with bootstrap_results


## could do this using modelr

```{r}
boot_straps=
  sim_df_constant |> 
  modelr::bootstrap(1000) |> 
  mutate(
    strap=map(strap, as_tibble),
    models=map(strap, \(df) lm(y~x, data=df)),
    results=map(models, broom::tidy)
  ) |> 
  select(.id, results) |> 
  unnest(results)
```
.id tells us first bootstrap sample, second bootstrap sample, etc.

## what do you want to report
```{r}
boot_straps |> 
  group_by(term) |> 
  summarize(
    boot_est=mean(estimate),
    boot_se=sd(estimate),
     boot_ci_ll=quantile(estimate, .025),
    boot_ci_ul=quantile(estimate, .975)
  )
```

## Airbnb

```{r}
data("nyc_airbnb")

manhattan_df = 
  nyc_airbnb |> 
  mutate(stars = review_scores_location / 2) |> 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) |> 
  filter(borough == "Manhattan") |> 
  select(price, stars, room_type) |> 
  drop_na()
```

plot the data
```{r}
manhattan_df |> 
  ggplot(aes(x=stars, y=price)) +
  geom_point()+
  stat_smooth(method= "lm", se=FALSE)
```
clearly not constant variance

fit a regression..but we dont trust the SE
```{r}
manhattan_df |> 
  lm(price~stars+room_type, data=_) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
```

bootstrap for better(?) inference

```{r}
boot_results=
manhattan_df |> 
  modelr::bootstrap(1000) |> 
  mutate(
    strap=map(strap, as_tibble),
    models=map(strap, \(df) lm(price~stars + room_type, data=df)),
    results=map(models, broom::tidy)
  ) |> 
  select(.id, results) |> 
  unnest(results)

boot_results |> 
  filter(term=="stars") |> 
  ggplot(aes(x=estimate))+
  geom_density()

boot_results |> 
  group_by(term) |> 
  summarize(
    boot_est=mean(estimate),
    boot_se=sd(estimate),
     boot_ci_ll=quantile(estimate, .025),
    boot_ci_ul=quantile(estimate, .975)
  )
  
```
taking 1000 bootstrap samples, fitted linear regressions of each of them, and each time i have estimated slope for effect of stars and we are plotting what this looks like. looks mostly like a normal distribution but left tail is a little long (can see large outliers having a slight effect)

now SE is still large for stars is still 6.19 (more variable than lm) bc it is a really skewed distrib in the residuals and we are getting results we dont expect
