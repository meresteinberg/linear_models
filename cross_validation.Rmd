---
title: "Cross Validation"
output: github_document
date: "2024-11-12"
---

```{r}
library(tidyverse)
library(modelr)
library(mgcv)

library(SemiPar)

set.seed(1)
```

Look at LIDAR data
```{r}
data("lidar")

lidar_df=
  lidar |> 
  as_tibble() |> 
  mutate(id=row_number())
```

```{r}
lidar_df |> 
  ggplot(aes(x=range, y=logratio))+
  geom_point()
```

## Try to do CV

We'll compare 3 models--one linear, one smooth, one wiggly.

Construct training and testing df
--test df is everything in lidar_df but not in train_df
```{r}
train_df=sample_frac(lidar_df, size=.8)
test_df=anti_join(lidar_df, train_df, by="id")
```

Look at these
```{r}
ggplot(train_df, aes(x=range, y=logratio))+
  geom_point()+
  geom_point(data = test_df, color="red")
```
--red points showing missing points in train_df 


Fit three models
```{r}
linear_mod=lm(logratio~range, data=train_df)
smooth_mod=gam(logratio~s(range), data=train_df)
wiggly_mod=gam(logratio~s(range, k=30), sp=10e-6, data=train_df)
```
--prof saying smooth will be right one

Look at fits
```{r}
train_df |> 
  add_predictions(linear_mod) |> 
  ggplot(aes(x=range, y=logratio))+
  geom_point()+
  geom_point(data = test_df, color="red")+
  geom_line(aes(y=pred, color="red"))

train_df |> 
  add_predictions(wiggly_mod) |> 
  ggplot(aes(x=range, y=logratio))+
  geom_point()+
  geom_point(data = test_df, color="red")+
  geom_line(aes(y=pred, color="red"))

train_df |> 
  add_predictions(smooth_mod) |> 
  ggplot(aes(x=range, y=logratio))+
  geom_point()+
  geom_point(data = test_df, color="red")+
  geom_line(aes(y=pred, color="red"))
```
---line not complex enough to capture this data
--wiggly line is doing a lot to try to follow: its too complex bc its chasing every fluctation in the dataset
--smooth mod is just right bc not doing too much


Compare these numerically using RMSE
```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```
---rmse smallest for smooth model (wiggly model is slightly higher)

## Repeat the train/test split




